{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/tentoilatai/Library/CloudStorage/OneDrive-Personal/Tran Viet Tai/Hoc tap/Ky VI/Hoc may va khai pha du lieu/BTL/GUIDE_Train.csv', nrows=10000)  # read a few rows to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['EvidenceRole'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['CountryCode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:,:15].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, [0, 9] + list(range(15, 30))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, [0, 9] + list(range(30, 45))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['IncidentGrade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['IncidentGrade'].value_counts() * 100 / train_data['IncidentGrade'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_cat_columns = ['Category', 'EntityType', 'EvidenceRole', 'SuspicionLevel', 'LastVerdict',\n",
    "                  'ResourceType', 'Roles', 'AntispamDirection', 'ThreatFamily','CountryCode',\n",
    "                  'OSFamily', 'OSVersion','State', 'City', 'RegistryValueName', 'RegistryValueData', \n",
    "                  'ResourceIdName', 'RegistryKey', 'OAuthApplicationId', 'ApplicationId', 'ApplicationName']\n",
    "\n",
    "numerical_columns = ['DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid', 'AccountUpn', 'AccountObjectId',\n",
    "                     'AccountName', 'DeviceName', 'NetworkMessageId', 'EmailClusterId', 'FileName', 'FolderPath']\n",
    "\n",
    "le_cat_columns += numerical_columns\n",
    "\n",
    "numerical_columns = []\n",
    "\n",
    "ohe_cat_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[le_cat_columns].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[numerical_columns].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data:\n",
    "    if train_data[col].nunique() < 10:\n",
    "        print(col, train_data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, le_cat_columns):\n",
    "    \"\"\"\n",
    "        This function preprocesses the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    for le_col in le_cat_columns:\n",
    "        df[le_col] = df[le_col].astype('object')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_data(train_data, le_cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[le_cat_columns].nunique())\n",
    "print(train_data[ohe_cat_columns].nunique())\n",
    "print(train_data[numerical_columns].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "cat_columns = ohe_cat_columns + le_cat_columns\n",
    "\n",
    "\n",
    "for cat in cat_columns:\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    X_encoded = onehot_encoder.fit_transform(train_data[[cat]])  \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(train_data['IncidentGrade'])\n",
    "    f_statistic, p_value = f_classif(X_encoded, y)\n",
    "    \n",
    "    print(\"*\" * 20)\n",
    "    print(f\"Feature: {cat}\")\n",
    "    print(f\"ANOVA F-Statistic: {f_statistic}\")\n",
    "    print(f\"p-Value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' column to datetime\n",
    "train_data['Timestamp'] = pd.to_datetime(train_data['Timestamp'])\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    train_data = pd.read_csv('/Users/tentoilatai/Library/CloudStorage/OneDrive-Personal/Tran Viet Tai/Hoc tap/Ky VI/Hoc may va khai pha du lieu/BTL/GUIDE_Train.csv') \n",
    "    test_data = pd.read_csv('/Users/tentoilatai/Library/CloudStorage/OneDrive-Personal/Tran Viet Tai/Hoc tap/Ky VI/Hoc may va khai pha du lieu/BTL/GUIDE_Test.csv')\n",
    "    \n",
    "    print(train_data.shape)\n",
    "    \n",
    "    train_data.dropna(subset=['IncidentGrade'], inplace=True)\n",
    "    \n",
    "    train_data = preprocess_data(train_data, le_cat_columns)\n",
    "    test_data = preprocess_data(test_data, le_cat_columns)\n",
    "    \n",
    "    group_columns = ohe_cat_columns + numerical_columns + le_cat_columns\n",
    "    \n",
    "    train_data = train_data.drop_duplicates(subset=group_columns)\n",
    "    \n",
    "    test_data.drop(['Usage'], axis=1, inplace=True)\n",
    "    \n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(train_data[ohe_cat_columns])\n",
    "\n",
    "    train_data_ohe = csr_matrix(ohe.transform(train_data[ohe_cat_columns]))\n",
    "    test_data_ohe = csr_matrix(ohe.transform(test_data[ohe_cat_columns]))\n",
    "\n",
    "    train_data_numerical = csr_matrix(train_data[numerical_columns].fillna(-1).values)\n",
    "    test_data_numerical = csr_matrix(test_data[numerical_columns].fillna(-1).values)\n",
    "    \n",
    "    feature_le = LabelEncoder()\n",
    "    \n",
    "    train_data_le = pd.DataFrame()\n",
    "    test_data_le = pd.DataFrame()\n",
    "    \n",
    "    for le_col in le_cat_columns:\n",
    "        feature_le.fit(pd.concat([train_data[le_col], test_data[le_col]]))\n",
    "        train_data_le[le_col] = feature_le.transform(train_data[le_col])\n",
    "        test_data_le[le_col] = feature_le.transform(test_data[le_col])\n",
    "    \n",
    "    train_data_le = csr_matrix(train_data_le)\n",
    "    test_data_le = csr_matrix(test_data_le)\n",
    "    \n",
    "    X_train = hstack([train_data_ohe, train_data_le ,train_data_numerical])\n",
    "    X_test = hstack([test_data_ohe, test_data_le, test_data_numerical])\n",
    "\n",
    "    target_le = LabelEncoder()\n",
    "    \n",
    "    target_le.fit(train_data['IncidentGrade'])\n",
    "    y_train = target_le.transform(train_data['IncidentGrade'])\n",
    "    y_test = target_le.transform(test_data['IncidentGrade'])\n",
    "    \n",
    "    \"\"\"\n",
    "        0: 'BenignPositive'\n",
    "        1: 'FalsePositive'\n",
    "        2: 'TruePositive'\n",
    "    \"\"\"\n",
    "    print(f\"Target Classes: {target_le.classes_}\")\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    \n",
    "X_train, y_train, X_test, y_test = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def predict(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, \n",
    "                                        display_labels = ['BenignPositive', 'FalsePositive', 'TruePositive'])\n",
    "\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_classifier(X_train, y_train):\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    feature_columns = np.array(ohe_cat_columns + le_cat_columns + numerical_columns)\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Feature Importances (Random Forest Classifier)\")\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model = train_random_forest_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(rfc_model, X_test, y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Macro-Precision: {}'.format(precision))\n",
    "print('Macro-Recall: {}'.format(recall))\n",
    "print('Macro-F1 Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_xgboost_classifier(X_train, y_train):\n",
    "    model = XGBClassifier(n_estimators=100, max_depth=5, random_state=0, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    feature_columns = np.array(ohe_cat_columns + le_cat_columns + numerical_columns)\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Feature Importances (XGBoost Classifier)\")\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = train_xgboost_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(xgb_model, X_test, y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Macro-Precision: {}'.format(precision))\n",
    "print('Macro-Recall: {}'.format(recall))\n",
    "print('Macro-F1 Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def train_catboost_classifier(X_train, y_train):\n",
    "    model = CatBoostClassifier(iterations=100, depth=5, random_seed=0, verbose=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    importances = model.get_feature_importance()\n",
    "    \n",
    "    feature_columns = np.array(ohe_cat_columns + le_cat_columns + numerical_columns)\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Feature Importances (CatBoost Classifier)\")\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = train_catboost_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(cat_model, X_test, y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Macro-Precision: {}'.format(precision))\n",
    "print('Macro-Recall: {}'.format(recall))\n",
    "print('Macro-F1 Score: {}'.format(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
